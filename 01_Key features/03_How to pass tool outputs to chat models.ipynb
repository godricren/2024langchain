{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to pass tool outputs to chat models\n",
    "\n",
    "Some models are capable of tool calling-generating arguments that conform to a specific user-provided schema. This guide will demonstrate how to use those tool calls to actually call a function and properly pass the results back to the model.\n",
    "\n",
    "![img](../asset/tool_invocation.png)\n",
    "\n",
    "First, let's define our tools and our model.\n"
   ],
   "id": "ce1c753ada036e2c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-16T23:37:27.373480Z",
     "start_time": "2024-09-16T23:37:27.092743Z"
    }
   },
   "source": [
    "import os\n",
    "import yaml\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "with open('../utils/config.yml', 'r') as stream:\n",
    "    zhipuai_api_key = yaml.safe_load(stream)['api_key']\n",
    "\n",
    "os.environ['ZHIPUAI_API_KEY'] = zhipuai_api_key\n",
    "\n",
    "chat_model = ChatZhipuAI(\n",
    "    model='glm-4-plus',\n",
    "    temperature=0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:37:30.426619Z",
     "start_time": "2024-09-16T23:37:30.362813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a:int, b:int):\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a:int, b:int):\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools=[add, multiply]\n",
    "\n",
    "llm_with_tools = chat_model.bind_tools(tools)"
   ],
   "id": "901a933e9d92063d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:37:36.196381Z",
     "start_time": "2024-09-16T23:37:34.694643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What's 3 * 12? Also, what is 11 + 49? \"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "print(ai_msg.tool_calls)\n",
    "messages.append(ai_msg)"
   ],
   "id": "eedd4b945ea09985",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_9021306401513663733', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_9021306401513663734', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next let's invoke the tool functions using the args the model populated!\n",
    "\n",
    "Conveniently, if we invoke a LangChain `Tool` with a `ToolCall`, we'll automatically get back a `ToolMessage` that can be feedback to the model:"
   ],
   "id": "4c264b401be86a75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:44:06.586197Z",
     "start_time": "2024-09-16T23:44:06.580546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {'add': add, 'multiply': multiply}[tool_call['name'].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "    \n",
    "messages"
   ],
   "id": "2ae6bb0d25fb42b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"What's 3 * 12? Also, what is 11 + 49? \"),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'multiply'}, 'id': 'call_9021306401513663733', 'index': 0, 'type': 'function'}, {'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'add'}, 'id': 'call_9021306401513663734', 'index': 1, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 282, 'total_tokens': 312}, 'model_name': 'glm-4-plus', 'finish_reason': 'tool_calls'}, id='run-ea205cb4-8e78-47ee-8734-36b29b20fedf-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_9021306401513663733', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_9021306401513663734', 'type': 'tool_call'}]),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='call_9021306401513663733'),\n",
       " ToolMessage(content='60', name='add', tool_call_id='call_9021306401513663734')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And finally, we'll invoke the model with the tool results. The model will use this information to generate a final answer to our original query:",
   "id": "aac69760438b3c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:47:24.060664Z",
     "start_time": "2024-09-16T23:47:22.249845Z"
    }
   },
   "cell_type": "code",
   "source": "llm_with_tools.invoke(messages)",
   "id": "aa08c97fd6f28fb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of 3 multiplied by 12 is 36, and the sum of 11 and 49 is 60.', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 290, 'total_tokens': 318}, 'model_name': 'glm-4-plus', 'finish_reason': 'stop'}, id='run-494c8b1d-0abb-445a-ba0e-b0a30a0f39c3-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that each `ToolMessage` must include a `tool_call_id`  that matches an `id` in the original tool calls that the model generates. This helps the model match tool response with tool calls.\n",
    "\n",
    "Tool calling agents, like those in LangGraph, use this basic flow to answer queries and solve tasks."
   ],
   "id": "7eff7ab694a452d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
