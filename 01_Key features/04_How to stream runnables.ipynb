{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to stream runnables\n",
    "\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
    "\n",
    "Important LangChain primitives like chat models, output parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.\n",
    "\n",
    "This interface provides two general approaches to stream content:\n",
    "\n",
    "1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n",
    "2. async `astream_events` and async `astream_log`: these provide a way to steam both **intermediate steps** and **final output** from the chain.\n",
    "\n",
    "Let's take a look at both approaches, and try to understand how to use them.\n",
    "\n",
    "\n",
    "## Using Stream\n",
    "\n",
    "All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n",
    "\n",
    "These methods are designed to stream the final output in chunks, yielding each chunk as soon as it available.\n",
    "\n",
    "Streaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n",
    "\n",
    "The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n",
    "\n",
    "The best place to exploring streaming is with the signle most important components in LLMs apps--the LLMs themselves!\n",
    "\n",
    "## LLMs and Chat Models\n",
    "\n",
    "Large language models and their chat variants are the primary bottleneck in LLM based apps.\n",
    "\n",
    "Large language modes can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n",
    "\n",
    "The key strategy to make the application feel more responsive is show intermediate progress; viz. to stream the output from the model **token by token**.\n",
    "\n",
    "We will show examples of streaming using a chat model."
   ],
   "id": "575031e2fa83f2ca"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-18T13:32:26.113445Z",
     "start_time": "2024-09-18T13:32:25.831886Z"
    }
   },
   "source": [
    "import os\n",
    "import yaml\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "with open('../utils/config.yml', 'r') as stream:\n",
    "    zhipuai_api_key = yaml.safe_load(stream)['api_key']\n",
    "\n",
    "os.environ['ZHIPUAI_API_KEY'] = zhipuai_api_key\n",
    "\n",
    "model = ChatZhipuAI(\n",
    "    model='glm-4-plus',\n",
    "    temperature=0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:33:52.557917Z",
     "start_time": "2024-09-18T13:33:44.750265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = []\n",
    "for chunk in model.stream('what color is the sky?'):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content,end='|', flush=True)"
   ],
   "id": "b59fc0dd9a152151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| color| of| the| sky| can| vary| depending| on| the| time| of| day|,| weather| conditions|,| and| the| observer|'s| location|.| Generally|:\n",
      "\n",
      "-| **|During| the| day|**:| The| sky| appears| blue| due| to| Ray|leigh| scattering|,| where| shorter| (|blue|)| wavelengths| of| sunlight| are| scattered| in| all| directions| by| the| gases| and| particles| in| the| Earth|'s| atmosphere|.\n",
      "\n",
      "-| **|At| sunrise| and| sunset|**:| The| sky| can| take| on| hues| of| orange|,| pink|,| and| red|.| This| occurs| because| the| sun| is| lower| on| the| horizon|,| and| its| light| has| to| pass| through| more| of| the| Earth|'s| atmosphere|,| scattering| the| shorter| wavelengths| and| allowing| the| longer| (|red| and| orange|)| wavelengths| to| dominate|.\n",
      "\n",
      "-| **|On| over|cast| days|**:| The| sky| might| appear| gray| or| white| due| to| the| presence| of| clouds| that| scatter| and| reflect| all| wavelengths| of| light| equally|.\n",
      "\n",
      "-| **|At| night|**:| The| sky| is| typically| dark|,| often| appearing| black| with| stars| visible|,| although| it| can| also| have| a| faint| blue| or| gray| tint| in| urban| areas| due| to| light| pollution|.\n",
      "\n",
      "These| are| general| descriptions|,| and| the| actual| color| can| vary| widely| based| on| specific| atmospheric| conditions|.||"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alternatively, if you're working in an async environment, you may consider using the async `astream` API:",
   "id": "e4e1a78105ee8b53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:37:59.391356Z",
     "start_time": "2024-09-18T13:37:48.519084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = []\n",
    "async for chunk in model.astream('what color is the sky?'):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content,end='|', flush=True)"
   ],
   "id": "134b434e16aa8a2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| color| of| the| sky| can| vary| depending| on| the| time| of| day|,| weather| conditions|,| and| the| observer|'s| location|.| During| the| day|,| when| the| sun| is| high| and| the| weather| is| clear|,| the| sky| typically| appears| blue|.| This| blue| color| is| due| to| Ray|leigh| scattering|,| where| the| Earth|'s| atmosphere| sc|atters| sunlight| in| all| directions| and| blue| light| is| scattered| more| because| it| travels| in| shorter|,| smaller| waves|.\n",
      "\n",
      "At| sunrise| and| sunset|,| the| sky| can| display| a| range| of| colors| including| red|s|,| oranges|,| and| pur|ples|.| This| happens| because| the| sun| is| lower| on| the| horizon|,| and| its| light| has| to| pass| through| a| thicker| layer| of| the| atmosphere|,| scattering| the| shorter|-w|avelength| light| (|blue| and| violet|)| out| of| our| line| of| sight| and| letting| the| red|s| and| oranges| dominate|.\n",
      "\n",
      "During| over|cast| or| storm|y| weather|,| the| sky| might| appear| gray| or| even| black|.| At| night|,| the| sky| is| generally| black|,| dotted| with| stars|,| although| it| can| also| appear| as| a| dark| blue| in| areas| with| significant| light| pollution|.\n",
      "\n",
      "In| certain| atmospheric| conditions|,| such| as| after| volcanic| eru|ptions| or| in| the| presence| of| specific| types| of| pollution|,| the| sky| can| take| on| other| hues|,| including| red|s|,| brow|ns|,| or| a| murky| gray|.\n",
      "\n",
      "So|,| while| the| most| common| color| associated| with| the| sky| is| blue|,| it| can| take| on| many| different| colors| based| on| various| factors|.||"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let inspect one of the chunks",
   "id": "529e396afbc5267a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:39:01.936378Z",
     "start_time": "2024-09-18T13:39:01.929874Z"
    }
   },
   "cell_type": "code",
   "source": "chunks[0]",
   "id": "bc5852b57072e66d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='The', id='run-516bff67-34e2-465d-a16a-6f3eb56f68b4')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We got back something called an `AIMessageChunk`. This chunk represents a prat of a `AIMessage`.\n",
    "\n",
    "Message chunks are additive by design==one can simply add them up to get the state f the response so far!"
   ],
   "id": "5eefdb8ce3b24246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:43:34.130025Z",
     "start_time": "2024-09-18T13:43:34.124576Z"
    }
   },
   "cell_type": "code",
   "source": "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]",
   "id": "ae8a52c537b983e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='The color of the sky', id='run-516bff67-34e2-465d-a16a-6f3eb56f68b4')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chains\n",
    "\n",
    "Virtually all LLM applications involve more steps than just a call to a language model.\n",
    "\n",
    "Let's build a simple chain using `LangChain Expression Language(LCEL)` that combines a prompt, model and a parser and verify that steaming works.\n",
    "\n",
    "We will use `StrOutputParser` to parse the output from the model. This is a simple parser that extracts the `content` field from an `AIMessageChunk`, giving us the `token` returned by the model."
   ],
   "id": "a7f6dbcf3e4d2e73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:53:27.681841Z",
     "start_time": "2024-09-18T13:53:25.995840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('tell me a joke about {topic}')\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({'topic': 'parrot'}):\n",
    "    print(chunk,end='|', flush=True)"
   ],
   "id": "822975852ced0dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure|,| here|'s| one| for| you|:\n",
      "\n",
      "Why| do| par|rots| only| ever| say| one| thing|?\n",
      "\n",
      "Because| they|'re| always| getting| called| to| \"|p|aws|\"|!| 🦜😄|"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that we're getting streaming output even though we're using `parser` at the end of the chain above. The `parser` operates on each streaming chunk individidually. Many of the LCEL primitives also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.\n",
    "\n",
    "Custom functions can be designed to return generators, which are able to operate on streams.\n",
    "\n",
    "Certain runnables, like prompt templates and chat models, cannot process individual chunks and instead aggregate all previous steps. Such runnable can interrupt the stream process"
   ],
   "id": "7172747eec2b4047"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
